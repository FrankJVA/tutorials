
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <meta http-equiv="refresh" content="0;url=https://training.h2o.ai/products/tutorial-1c-machine-learning-interpretability-tutorial">       
  <link rel="canonical" href="https://training.h2o.ai/products/tutorial-1c-machine-learning-interpretability-tutorial">
  <title>Machine Learning Interpretability Tutorial</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="../assets/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab title="Machine Learning Interpretability Tutorial"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Objective" duration="0">
        <p>As the field of machine learning continues to grow, more industries from healthcare to banking are adopting machine learning models to generate predictions. These predictions are being used to justify the cost of healthcare and for loan approvals or denials. For Regulated industries that are adopting machine learning, <strong>interpretability</strong> is a requirement. In <a href="https://arxiv.org/pdf/1702.08608.pdf" target="_blank">&#34;Towards a rigorous science of interpretable machine learning&#34;</a> by Finale Doshi-Velez and Been Kim&#34; interpretability, they define interpretability in the context of ML systems as &#34;the ability to explain or to present in understandable terms to a human&#34;. This is a straightforward definition of interpretability. See the Deeper Dive and Resource section of the Goal section to read more about other takes on  interpretability.</p>
<p>The motivations for interpretability are:</p>
<ul>
<li>Better human understanding of impactful technologies</li>
<li>Regulation compliance and GDPR &#34;Right to explanation&#34;</li>
<li>Check and balance against accidental or intentional discrimination</li>
<li>Hacking and adversarial attacks</li>
<li>Alignment with US FTC and OMB guidance on transparency and explainability</li>
<li>Prevent building excessive machine learning technical debt</li>
<li>Drive deeper insight to understanding your data through better understanding of your models<br><br><br></li>
</ul>
<p>In this tutorial, we will generate a machine learning model using an example financial dataset and explore some of the most popular ways to interpret a generated machine learning model. Furthermore, we will learn to interpret the results, graphs, scores and reason code values of H2O Driverless AI generated models.</p>
<p><strong>Note:</strong> We recommend that you go over the entire tutorial first to review all the concepts, that way, once you start the experiment, you will be more familiar with the content.</p>
<h2>Deeper Diver and Resources</h2>
<p><strong>Learn more about Interpretability</strong>:</p>
<ul>
<li>Finale Doshi-Velez and Been Kim. <a href="https://arxiv.org/pdf/1702.08608.pdf" target="_blank">&#34;Towards a rigorous science of interpretable machine learning&#34;</a></li>
<li><a href="http://www.fatml.org/resources/principles-for-accountable-algorithms" target="_blank">FAT/ML</a></li>
<li><a href="https://www.darpa.mil/program/explainable-artificial-intelligence" target="_blank">XAI</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Prerequisites" duration="0">
        <ul>
<li>Basic knowledge of Machine Learning and Statistics</li>
<li>Basic knowledge of Driverless AI or doing the <a href="https://training.h2o.ai/products/tutorial-1a-automatic-machine-learning-introduction-with-driverless-ai" target="_blank">Automatic Machine Learning Introduction with Drivereless AI Test Drive</a></li>
<li>A <strong>Two-Hour Test Drive session</strong> : Test Drive is H2O.ai&#39;s Driverless AI on the AWS Cloud. No need to download software. Explore all the features and benefits of the H2O Automatic Learning Platform.<ul>
<li>Need a <strong>Two-Hour Test Drive</strong> session? Follow the instructions on <a href="https://training.h2o.ai/products/tutorial-0-getting-started-with-driverless-ai-test-drive" target="_blank">this quick tutorial</a> to get a Test Drive session started.<br></li>
</ul>
</li>
</ul>
<p><strong>Note:  Aquarium&#39;s Driverless AI Test Drive lab has a license key built-in, so you don&#39;t need to request one to use it. Each Driverless AI Test Drive instance will be available to you for two hours, after which it will terminate. No work will be saved. If you need more time to further explore Driverless AI, you can always launch another Test Drive instance or reach out to our sales team via the </strong><a href="https://www.h2o.ai/company/contact/" target="_blank"><strong>contact us form</strong></a><strong>.</strong></p>


      </google-codelab-step>
    
      <google-codelab-step label="Task 1: Launch Experiment and MLI" duration="0">
        <h2>About the Dataset</h2>
<p>This dataset contains information about credit card clients in Taiwan from April 2005 to September 2005. Features include demographic factors, repayment statuses, history of payment, bill statements and default payments.</p>
<p>The data set comes from the <a href="https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#" target="_blank">UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science</a></p>
<p>This dataset has a total 25 Features(columns) and 30,000 Clients(rows).</p>
<h2>Download Dataset</h2>
<p>1. Go to our S3 link <a href="https://s3.amazonaws.com/data.h2o.ai/DAI-Tutorials/TestDrive-Datasets/UCI_Credit_Card.csv" target="_blank">UCI_Credit_Card.csv</a> and download the file to your local drive.</p>
<h2>Launch Experiment</h2>
<p>1. Load the UCI_Credit_Card.csv to Driverless AI by clicking <strong>Add Dataset (or Drag and Drop)</strong> on the <strong>Datasets overview</strong> page.</p>
<p>2. Click on the <code>UCI_Credit_Card.csv</code> file then select <strong>Details</strong>.</p>
<p><img alt="dataset-details-selection" src="img/d2fb3969001b3a43.jpg"></p>
<p>Let&#39;s have a look at the columns:</p>
<p><img alt="dataset-details-page" src="img/dfa42ba9768ac8c3.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li><strong>ID</strong> - Row identifier (which will not be used for this experiment)</li>
<li><strong>LIMIT_BAL</strong> - Credit Limit</li>
<li><strong>Sex</strong></li>
<li><strong>EDUCATION</strong>- Education Level</li>
<li><strong>MARRIAGE</strong> - Marital Status</li>
<li><strong>Age</strong></li>
<li><strong>PAY0-PAY6</strong>: Payment Information including most recent repayment status<br> Status Values:<br>-2: Paid in full<br>-1: Paid with another line of credit<br>0: No consumption<br>1: 1 Month late<br>2: 2 Months late<br>Up to 9 Months late</li>
</ol>
<p>3. Continue scrolling the current page to see more columns.(Image is not included.)</p>
<ol type="1">
<li><strong>BILL_AMT0-BILL_AMT6</strong> - Recent credit card bills up to 6 months ago</li>
<li><strong>PAYAMT0-_PAY_AMT6</strong> - Recent payment towards their bill up to 6 months ago</li>
<li><strong>default.payment.next.month</strong>- Prediction of whether someone will default on the next month&#39;s payment using given information.</li>
</ol>
<p>4. Return to the <strong>Datasets</strong> page.</p>
<p>5. Click on the <strong>UCI_Credit_Card.csv</strong> file then select <strong>Predict</strong>.</p>
<p>6. Select <strong>Not Now</strong> on the <strong>First time Driverless AI, Click Yes to get a tour!</strong>. A similar image should appear:</p>
<p><img alt="experiment-page" src="img/7676f1f33b98c3cb.jpg"></p>
<p>Name your experiment <code>UCI_CC Classification Tutorial</code></p>
<p>7. Select <strong>Target Column</strong>, then Select <strong>default.payment.next.month</strong> as the target.</p>
<p><img alt="select-default-payment-next-month-column" src="img/cf667df90b4eba99.jpg"></p>
<p>8. <strong>Launch Experiment</strong> using the default settings suggested by Driverless AI.</p>
<p><img alt="run-experiment-default-setup" src="img/23cd31ceed4c5838.jpg"></p>
<p>9. While on the <strong>Experiment</strong> page:</p>
<p><img alt="experiment-page-brief-explanation" src="img/32438c3353ff0237.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li><strong>Interpretability</strong>  - The interpretability knob is adjustable. The higher the interpretability, the simpler the features the main modeling routine will extract from the dataset.<br>If the interpretability is high enough then a monotonically constrained model will be generated.</li>
<li><strong>Time</strong> - The number of generations a genetic algorithm you can wait for. Higher the time the longer the wait since Driverless AI will work to engineer many features.</li>
<li><strong>Accuracy</strong> -  The complexity of the underlying models. High accuracy will build complex underlying models.</li>
<li><strong>Notifications</strong> - System notifications</li>
<li><strong>Variable Importance</strong> - Engineered features</li>
</ol>
<p>10. Select <strong>Interpret this Model</strong></p>
<p><img alt="select-interpret-this-model" src="img/a6f70e51c49a2eea.jpg"></p>
<p>While the model is being interpreted an image similar to the one below will appear. The goal is to explain the model that was just trained, which will take a few minutes.</p>
<p><img alt="mli-interpret-model" src="img/f39f544e609b9d45.jpg"></p>
<p>11. Once the <strong>MLI Experiment is Finished</strong> page comes up, select <strong>Yes</strong>, an image similar to the one below will appear:</p>
<p><img alt="mli-regression-and-classification-explanations-1" src="img/e8c0b812aa94e5c4.jpg"></p>
<p><img alt="mli-regression-and-classification-explanations-2" src="img/4723813ac51abe5.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>Summary of some basic facts about the model</li>
<li>Ranked variable importance in the space of the derived features (harder to understand)</li>
<li>Accuracy of surrogate models, or simple models of complex models</li>
<li>Ranked variable importance in the space of the original features (easier to understand)</li>
</ol>
<p>Notice that some of the highly ranked variables of the original features (4) show up also as highly ranked variables of the derived features (2). The ranked original variable importance (4) can be used to reason through the more complex features in (2).</p>


      </google-codelab-step>
    
      <google-codelab-step label="Task 2: Industry Context and ML Explainability Concepts" duration="0">
        <h2>Responsibility in AI and Machine Learning</h2>
<p>The explainability and interpretability in the machine learning space has grown a tremendous amount since we first developed DriverlessAI, with that in mind it is important to frame the larger context in which our interpretability toolkit falls within. It is also worth noting that since this first training was developed, the push toward regulation, oversight, and auditing of ML models and the companies who deploy them has moved rapidly, making these techniques critical requirements for firms looking to make artificial intelligence a part of their companies operations going forward. There have been many recent developments globally, which we have linked below, but the consistent themes seen are fairness, transparency, explainability, interpretability, privacy, and security. We have defined a handful of these terms below.</p>
<p><img alt="task-2-venn-diagram" src="img/d4fa66f14f4f9ff7.jpg"></p>
<ul>
<li>Explainable AI (XAI):  The ability to explain a model after it has been developed.</li>
<li>Interpretable Machine Learning:  Transparent model architectures and increasing how intuitive and understandable ML models can be.</li>
<li>Ethical AI:  Sociological fairness in machine learning predictions (i.e., whether one category of person is being weighted unequally).</li>
<li>Secure AI:  Debugging and deploying ML models with similar counter-measures against insider and cyber threats as would be seen in traditional software.</li>
<li>Human-Centered AI:  User interactions with AI and ML systems.</li>
<li>Compliance:Whether that&#39;s with GDPR, CCPA, FCRA, ECOA or other regulations, as an additional and crucial aspect of responsible AI.</li>
</ul>
<h2>Machine Learning Interpretability Taxonomy</h2>
<p>In the context of machine learning models and results, interpretability has been defined as the ability to explain or to present in understandable terms to a human [1]. Of course, interpretability and explanations are subjective and complicated subjects, and a previously defined taxonomy has proven useful for characterizing interpretability in greater detail for various explanatory techniques [2]. Following Ideas on Interpreting Machine Learning, presented approaches will be described in technical terms but also in terms of response function complexity, scope, application domain, understanding, and trust.</p>
<h2>Response Function Complexity</h2>
<p>The more complex a function, the more difficult it is to explain. Simple functions can be used to explain more complex functions, and not all explanatory techniques are a good match for all types of models. Hence, it&#39;s convenient to have a classification system for response function complexity.</p>
<p><strong>Linear, monotonic functions</strong>: Response functions created by linear regression algorithms are probably the most popular, accountable, and transparent class of machine learning models. These models will be referred to here as linear and monotonic. They are transparent because changing any given input feature (or sometimes a combination or function of an input feature) changes the response function output at a defined rate, in only one direction, and at a magnitude represented by a readily available coefficient. Monotonicity also enables accountability through intuitive, and even automatic, reasoning about predictions. For instance, if a lender rejects a credit card application, they can say exactly why because their probability of default model often assumes that credit scores, account balances, and the length of credit history are linearly and monotonically related to the ability to pay a credit card bill. When these explanations are created automatically and listed in plain English, they are typically called reason codes. In Driverless AI, linear and monotonic functions are fit to very complex machine learning models to generate reason codes using a technique known as K-LIME discussed in <strong>Task 6</strong>.</p>
<p><strong>Nonlinear, monotonic functions</strong>: Although most machine learned response functions are nonlinear, some can be constrained to be monotonic with respect to any given input feature. While there is no single coefficient that represents the change in the response function induced by a change in a single input feature, nonlinear and monotonic functions are fairly transparent because their output always changes in one direction as a single input feature changes.</p>
<p><strong>Nonlinear, monotonic response functions</strong> also enable accountability through the generation of both reason codes and feature importance measures. Moreover, nonlinear, monotonic response functions may even be suitable for use in regulated applications. In Driverless AI, users may soon be able to train nonlinear, monotonic models for additional interpretability. Nonlinear, non-monotonic functions: Most machine learning algorithms create nonlinear, non-monotonic response functions. This class of functions is typically the least transparent and accountable of the three classes of functions discussed here. Their output can change in a positive or negative direction and at a varying rate for any change in an input feature. Typically, the only standard transparency measure these functions provide are global feature importance measures. By default, Driverless AI trains nonlinear, non-monotonic functions.</p>
<h2>Scope</h2>
<p>Traditional linear models are globally interpretable because they exhibit the same functional behavior throughout their entire domain and range. Machine learning models learn local patterns in training data and represent these patterns through complex behavior in learned response functions. Therefore, machine-learned response functions may not be globally interpretable, or global interpretations of machine-learned functions may be approximate. In many cases, local explanations for complex functions may be more accurate or simply more desirable due to their ability to describe single predictions.</p>
<p><strong>Global Interpretability</strong>: Some of the presented techniques facilitate global transparency in machine learning algorithms, their results, or the machine-learned relationship between the inputs and the target feature. Global interpretations help us understand the entire relationship modeled by the trained response function, but global interpretations can be approximate or based on averages.</p>
<p><strong>Local Interpretability</strong>: Local interpretations promote understanding of small regions of the trained response function, such as clusters of input records and their corresponding predictions, deciles of predictions and their corresponding input observations, or even single predictions. Because small sections of the response function are more likely to be linear, monotonic, or otherwise well-behaved, local explanations can be more accurate than global explanations.</p>
<p><strong>Global Versus Local Analysis Motif</strong>: Driverless AI provides both global and local explanations for complex, nonlinear, non-monotonic machine learning models. Reasoning about the accountability and trustworthiness of such complex functions can be difficult, but comparing global versus local behavior is often a productive starting point. A few examples of global versus local investigation include:</p>
<ul>
<li>For observations with globally extreme predictions, determine if their local explanations justify their extreme predictions or probabilities.</li>
<li>For observations with local explanations that differ drastically from global explanations, determine if their local explanations are reasonable.</li>
<li>For observations with globally median predictions or probabilities, analyze whether their local behavior is similar to the model&#39;s global behavior.</li>
</ul>
<h2>Application Domain</h2>
<p>Another important way to classify interpretability techniques is to determine whether they are model-agnostic (meaning they can be applied to different types of machine learning algorithms) or model-specific (meaning techniques that are only applicable for a single type or class of algorithms). In Driverless AI, decision tree surrogate, ICE, K-LIME, and partial dependence are all model-agnostic techniques, whereas Shapley, LOCO, and random forest feature importance are model-specific techniques.</p>
<h2>Understanding and Trust</h2>
<p>Machine learning algorithms and the functions they create during training are sophisticated, intricate, and opaque. Humans who would like to use these models have basic, emotional needs to understand and trust them because we rely on them for our livelihoods or because we need them to make important decisions for us. The techniques in Driverless AI enhance understanding and transparency by providing specific insights into the mechanisms and results of the generated model and its predictions. The techniques described here enhance trust, accountability, and fairness by enabling users to compare model mechanisms and results to domain expertise or reasonable expectations and by allowing users to observe or ensure the stability of the Driverless AI model.</p>
<h2>The Multiplicity of Good Models</h2>
<p>It is well understood that for the same set of input features and prediction targets, complex machine learning algorithms can produce multiple accurate models with very similar, but not the same, internal architectures [3]. This alone is an obstacle to interpretation, but when using these types of algorithms as interpretation tools or with interpretation tools, it is important to remember that details of explanations can change across multiple accurate models. This instability of explanations is a driving factor behind the presentation of multiple explanatory results in Driverless AI, enabling users to find explanatory information that is consistent across multiple modeling and interpretation techniques.</p>
<h2>References</h2>
<p>[1] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiV preprint, 2017</p>
<p>[2] <a href="https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning" target="_blank">Patrick Hall, Wen Phan, and Sri Satish Ambati. Ideas on interpreting machine learning. O&#39;Reilly Ideas, 2017</a></p>
<p>[3] <a href="https://projecteuclid.org/euclid.ss/1009213726" target="_blank">Leo Breiman. Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 2001.</a></p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf" target="_blank">Hall, P., Gill, N., Kurka, M., Phan, W. (Jan 2019). Machine Learning Interpretability with H2O Driverless AI.</a></li>
<li><a href="https://arxiv.org/abs/1810.02909" target="_blank">On the Art and Science of Machine Learning Explanations</a></li>
<li><a href="https://www.h2o.ai/oreilly-mli-booklet-2019/" target="_blank">An Introduction to Machine Learning Interpretability Second Edition</a> (2019)</li>
<li><a href="https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques" target="_blank">Testing machine learning explanation techniques</a></li>
<li><a href="https://github.com/jphall663/awesome-machine-learning-interpretability" target="_blank">Awesome Machine Learning Interpretability</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/references.html" target="_blank">Concept References</a></li>
<li><a href="https://www.ftc.gov/news-events/blogs/business-blog/2020/04/using-artificial-intelligence-algorithms" target="_blank">Using Artificial Intelligence and Algorithms</a></li>
<li><a href="https://www.finra.org/sites/default/files/2020-06/ai-report-061020.pdf" target="_blank">A REPORT FROM THE FINANCIAL INDUSTRY REGULATORY AUTHORITY</a></li>
<li><a href="https://www.whitehouse.gov/wp-content/uploads/2020/01/Draft-OMB-Memo-on-Regulation-of-AI-1-7-19.pdf" target="_blank">MEMORANDUM FOR THE HEADS OF EXECUTIVE DEPARTMENTS AND AGENCIES</a></li>
<li><a href="https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgmodelaigovframework2.pdf" target="_blank">MODEL ARTIFICIAL INTELLIGENCE GOVERNANCE FRAMEWORK SECOND EDITION</a></li>
<li><a href="https://gdpr-info.eu/" target="_blank">General Data Protection Regulation: GDPR</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 3: Global Shapley Values and Feature Importance" duration="0">
        <h2>Global Shapley Values and Feature Importance Concepts</h2>
<p>Global Shapley values are the average of the local Shapley values over every row of a data set. Feature importance measures the effect that a feature has on the predictions of a model. Global feature importance measures the overall impact of an input feature on the Driverless AI model predictions while taking nonlinearity and interactions into consideration.</p>
<p>Global feature importance values give an indication of the magnitude of a feature&#39;s contribution to model predictions for all observations. Unlike regression parameters or average Shapley values, they are often unsigned and typically not directly related to the numerical predictions of the model.</p>
<p>(In  contrast, local feature importance describes how the combination of the learned model rules or parameters and an individual observation&#39;s attributes affect a model&#39;s prediction for that observation while taking nonlinearity and interactions into effect.)</p>
<ul>
<li>Scope of Interpretability.<br>(1) Random forest feature importance is a global interpretability measure. (2) Shapley and LOCO feature importance is a local interpretability measure but can be aggregated to become global.</li>
<li>Appropriate Response Function Complexity.Random forest, Shapley, and LOCO feature importance can be used to explain tree-based response functions of nearly any complexity.<ul>
<li>Understanding and Trust. (1) Random forest feature importance and global Shapley feature importance increases transparency by reporting and ranking influential input features (2) Local Shapley and LOCO feature importance enhances accountability by creating explanations for each model prediction. (3) Both global and local feature importance enhance trust and fairness when reported values conform to human domain knowledge and reasonable expectations.</li>
<li>Application Domain. (1) Tree SHAP and Random forest feature importance is a model specific explanatory technique. (2) LOCO is a model-agnostic concept, but its implementation in Driverless AI is model specific.</li>
</ul>
</li>
</ul>
<h2>Shapley and Feature Importance Plots</h2>
<p>1. On the right-upper corner of the MLI page, select <strong>Driveless AI Model</strong>, then <strong>Shapley</strong>.</p>
<p><img alt="global-shapley-feature-importance" src="img/c2e436bb40e95b82.jpg"></p>
<p>The plot above is a sample of a Shapley plot. Shapley is an &#34;old&#34;, very advanced tool, now being applied to machine learning. This plot shows the global  importance value of the derived features. Notice the feature importance values are signed (scroll down to see the rest of the Shapley plot). The sign determines in which direction the values impact the model predictions on average. Shapley plots help by providing accurate and consistent variable importance even if data changes slightly.</p>
<p>Viewing the Global Shapley values plot is good place to start  because it provides a global view of feature importance and we can see which features are driving the model from an overall perspective.</p>
<p>Derived features can be difficult to understand. For that reason, it also helps to look at this complex system from the space of the original inputs, surrogate models allows to us to do this.</p>
<p>2. Click on <strong>Surrogate Models</strong>, <strong>Random Forest</strong>, then <strong>Feature Importance</strong>.</p>
<p><img alt="feature-importance-plot" src="img/c5e9d5b76d5dbaa3.jpg"></p>
<p>The <strong>Feature Importance</strong> plot ranks the original features. These features are the original drivers of the model in the original feature space. These values were calculated by building  a <strong>Random Forest</strong> between the original features and the predictions of the complex driverless AI model that was just trained.</p>
<p>3. Go back to <strong>Summary</strong>. Scroll down the page until you can view the <strong>Surrogate Model Summary</strong> &gt; <strong>Random Forest</strong> summary:</p>
<p><img alt="summary-random-forest" src="img/71a6f4b77ca8f0e5.jpg"></p>
<p>This single <strong>Random Forest</strong> model of a complex Driverless AI model is very helpful because we can see that this is a trustworthy model between the original inputs to the system and the predictions of the system. Note the low mean squared error(0.0407), high R2 (96%).</p>
<p>4. Go back to the Shapley plot and find the feature importance of LIMIT_BAL. How important was LIMIT_BAL in the global feature importance space? Was LIMIT_BAL a main driver in this space?</p>
<p>5. Look for LIMIT_BAL in the <strong>Feature Importance</strong> under <strong>Surrogate Models</strong>. How important was LIMIT_BAL in the original feature importance space? Was LIMIT_BAL a main driver in this space?</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#understanding-the-model-interpretation-page" target="_blank">Understanding the Model Interpretation Page</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 4: Partial Dependence Plot" duration="0">
        <h2>Partial Dependence Concepts</h2>
<p>Partial dependence is a measure of the average model prediction with respect to an input variable. In other words, the average prediction of the model with respect to the values of any one variable. Partial dependence plots display how machine-learned response functions change based on the values of an input variable of interest, while taking nonlinearity into consideration and averaging out the effects of all other input variables.</p>
<p>Partial dependence plots are well-known and described in the Elements of Statistical Learning (Hastie et al, 2001). Partial dependence plots enable increased transparency in Driverless AI models and the ability to validate and debug Driverless AI models by comparing a variable&#39;s average predictions across its domain to known standards, domain knowledge, and reasonable expectations.</p>
<h2>Partial Dependence Plot</h2>
<p>Through the <strong>Shapley Values</strong> and <strong>Feature Importance</strong>, we got a global perspective of the model. Now we will explore the global behavior of the features with respect to the model. This is done through the Partial Dependency Plot</p>
<p>1. Select <strong>Surrogate Models</strong>, <strong>Random Forest</strong> then <strong>Partial Dependecy Plot</strong></p>
<p><img alt="partial-dependency-plot" src="img/271060e527b11132.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>These values of PAY_0  represent the average predictions of all persons that paid on time or did not use their credit card</li>
<li>This value represents the average prediction of persons who were late one month for PAY_0</li>
<li>PAY_0 =2 has an average default probability of 0.602, then the default probability slowly and slightly decreases all the way to month 8.</li>
</ol>
<p>The results indicate that overall, in the entire dataset, the worst thing for a person to be in regarding defaulting with respect to PAY_0 is to be two months late. This behavior insight needs to be judged by the user who can determine whether this model should be trusted.</p>
<p>2. A good question to ask here is, is it worse to be two months late than being eight months late on your credit card bill? We might look to see a spike at month 2, then slight increases with each following month.</p>
<p>3. Explore the partial dependence for <strong>Pay_2</strong> by changing the <strong>PDP Variable</strong> at the upper-left side of the <strong>Partial Dependence Plot</strong> to <strong>Pay_2</strong></p>
<p><img alt="partial-dependence-pdp-variable" src="img/4b867acced17d026.jpg"></p>
<p>4.  What is the average predicted default probability for PAY_2 = 2?</p>
<p>5. Explore the partial dependence for <strong>LIMIT_BAL</strong> by changing the <strong>PDP Variable</strong> at the upper-left side of the <strong>Partial Dependence Plot</strong> to <strong>LIMIT_BAL</strong>, then hovering over the yellow circles.</p>
<p><img alt="partial-dependence-pdp-variable-lmt-bal" src="img/eb4ed1b6b0f342dc.jpg"></p>
<p>The grey area is the standard deviation of the partial dependence. The wider the standard deviation, the less trustworthy the average behavior (yellow line) is. In this case, the standard deviation follows the average behavior and is narrow enough, therefore trustworthy.</p>
<p>6. What is the average default probability for the lowest credit limit? How about for the highest credit limit?</p>
<p>7. What seems to be the trend regarding credit limit and a person defaulting on their payments?</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#partial-dependence-and-individual-conditional-expectation-ice" target="_blank">Partial Dependence and Individual Conditional Expectation (ICE)</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 5: Decision Tree Surrogate" duration="0">
        <h2>Decision Tree Surrogate Concepts</h2>
<ul>
<li>Scope of Interpretability. (1) Generally, decision tree surrogates provide global interpretability. (2) The attributes of a decision tree are used to explain global attributes of a complex Driverless AI model such as important features, interactions, and decision processes.</li>
<li>Appropriate Response Function Complexity. Decision tree surrogate models can create explanations for models of nearly any complexity.</li>
<li>Understanding and Trust. (1) Decision tree surrogate models foster understanding and transparency because they provide insight into the internal mechanisms of complex models. (2) They enhance trust, accountability, and fairness when their important features, interactions, and decision paths are in line with human domain knowledge and reasonable expectations.</li>
<li>Application Domain. Decision tree surrogate models are model agnostic</li>
</ul>
<h2>Decision Tree</h2>
<p>Now we are going to gain some insights into interactions. There are two ways in Driverless AI to do this, one of them is the <strong>Decision Tree</strong>. A <strong>Decision Tree</strong> is another surrogate model.</p>
<p>1. Select <strong>Surrogate Models</strong>, then <strong>Decision Tree</strong></p>
<p><img alt="decision-tree" src="img/8c7e77e7c5f37279.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>The RMSE value is low and the R2 value is fairly high</li>
<li>The values at the top of the <strong>Decision Tree</strong> are those variables of higher importance.</li>
</ol>
<p>Variables below one-another in the surrogate decision tree may also have strong interactions in the Driverless AI model.</p>
<p>Based on the low RMSE and the fairly high R2, it can be concluded that this is a somewhat trustworthy surrogate model. This single decision tree, provides an approximate overall flow chart of the complex model&#39;s behavior.</p>
<p>2. What are the most important variables in the <strong>Decision Tree</strong>? How do those variables compare to the previous plots we have analyzed?</p>
<p>A potential interaction happens when a variable is below another variable in the decision tree. In the image below, an potential interaction is observed between variables PAY_0 and PAY_2.</p>
<p><img alt="decision-tree-variable-interaction" src="img/2b24f4701aec1172.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>Potential Interaction between PAY_0 and PAY_2. This observation can be strengthened by looking at the Shapley Plot and locating any PAY_0 and PAY_2 interactions.</li>
<li>The thickness of the yellow line indicates that this is the most common path through the decision tree. This path is the lowest probability of default leaf node. Does this make sense from a business perspective?</li>
</ol>
<p>It can be observed from the <strong>Decision Tree</strong> that most people tend to pay their bills on time based on the thickness of the path highlighted with green arrows. The people in the highlighted path, are those with the lowest default probability. This low default probability path on the <strong>Decision Tree</strong> is an approximation to how the complex model would place people in a low default probability &#34;bucket&#34;.</p>
<p>Those who landed in that low default probability bucket had to have made their two most recent payments and have a  most recent bill payment of greater than or equal to 2168.500 Taiwanese dollars.</p>
<p>The path to a low default probability can be confirmed by looking at the <strong>Partial Dependency</strong> plots (image below) for both PAY_0 and PAY_2. Both plots confirm the low default probability before month two. Below is the <strong>Partial Dependency</strong> PAY_0 plot with the low default probability values highlighted.</p>
<p><img alt="partial-dependency-pay-0" src="img/3070d716c2f3041f.jpg"></p>
<p>It is important to note that what we are confirming is not whether the model&#39;s results are &#34;correct&#34; rather how is the model behaving. The model needs to be analyzed, and decisions need to be made about whether or not the model&#39;s behavior is correct.</p>
<p><img alt="decision-tree-low-and-high-default-paths" src="img/fb6a5044649d5cfe.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li><strong>Decision Tree</strong> path for low default probability</li>
<li><strong>Decision Tree</strong> path for high default probability.</li>
</ol>
<p>Based on the <strong>Decision Tree</strong>, to end up at the high probability of default bucket, a person would need to miss the first payment (PAY_0), be late on PAY_6, and be late on PAY_2. A history of poor repayment behavior from 6 months ago would more than likely place a person down the path of defaulting on their total bill payment. Just like the path with low default probability, the behavior of the variables for the high default probability need to be analyzed and ensure that their interactions and conclusions make sense.</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#decision-tree" target="_blank">Decision Tree</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 6: K-LIME" duration="0">
        <h2>K-LIME Concepts</h2>
<p>K-LIME is a variant of the LIME technique proposed by Ribeiro et al (2016). K-LIME generates global and local explanations that increase the transparency of the Driverless AI model, and allow model behavior to be validated and debugged by analyzing the provided plots, and comparing global and local explanations to one-another, to known standards, to domain knowledge, and to reasonable expectations.</p>
<ul>
<li>Scope of Interpretability<br>K-LIME provides several different scales of interpretability: (1) coefficients of the global GLM surrogate provide information about global, average trends, (2) coefficients of in-segment GLM surrogates display average trends in local regions, and (3) when evaluated for specific in-segment observations, K-LIME provides reason codes on a pre-observation basis.</li>
<li>Appropriate Response Function Complexity<br>(1) K-LIME can create explanations for machine learning models of high complexity. (2) K-LIME accuracy can decrease when the Driverless AI model becomes too nonlinear.</li>
<li>Understanding and Trust<br>(1) K-LIME increases transparency by revealing important input features and their linear trends. (2) K-LIME enhances accountability by creating explanations for each observation in a dataset. (3) K-LIME bolsters trust and fairness when the important features and their linear trends around specific records conform to human domain knowledge and reasonable expectations.</li>
<li>Application Domain. K-LIME is model agnostic.</li>
</ul>
<h2>K-LIME Plot</h2>
<p>The recent tasks have focused on the model&#39;s global behavior for the entire dataset, but how does the model behave for a single person? A great but complex tool for this is K-Lime.</p>
<p>1. Under <strong>Surrogate Models</strong>, select <strong>K-LIME</strong>.</p>
<p><img alt="k-lime-plot" src="img/296ea4e1f87d6043.jpg"></p>
<p>2. On the green highlighted area of the K-LIME Plot click on <strong>Model Prediction</strong>, <strong>LIME Model Prediction</strong> then <strong>Actual Target</strong> . The K-LIME plot should look similar to the image below:</p>
<p><img alt="empty-k-lime-plot" src="img/1882d9f1d5b38792.jpg"></p>
<p>3. Click on <strong>Model Prediction</strong> again, this time the plot will look similar to the one below:</p>
<p><img alt="k-lime-model-prediction" src="img/21e99cc3177eb3e3.jpg"></p>
<p><em>Things to note</em>:</p>
<p>This plot is the predictions of the Driverless AI model from lowest to highest.<br>The x-axis is the index of the rows that causes that ranking to occur from lowest to highest.</p>
<p>4. Add <strong>Actual Target</strong> by clicking on it, the plot should look similar to the one below:</p>
<p><img alt="k-lime-actual-target" src="img/2ab1235d4bb0f3.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>People who did not pay their bills on time</li>
<li>People who paid their bills on time</li>
</ol>
<p>Adding the <strong>Actual Target</strong> to the plot allows us to check if the model is not entirely wrong. From the plot,  the density of  line (2) near the low ranked predictions show that many people made their payments on time while those in line (1) had missed payments since the line is scattered. Towards the high ranked predictions, the density of line (1) shows the high likelihood of missing payments while the sparseness of line (2) shows those who have stopped making payments. These observations are a good sanity check.</p>
<p>5. Now, click on <strong>LIME Model Prediction</strong>.</p>
<p><img alt="k-lime-lime-model-prediction" src="img/f9ad29cd23ce9cf8.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>The high value of R2=88% and low RMSE=0.0662 value show that this is a highly accurate linear model. In other words this surrogate model is good enough to proceed, and explains about 90% of the variance in the Driverless AI model predictions.</li>
</ol>
<p>This single linear model trained on the original inputs of the system to predict the predictions of the Driverless AI model shows that the Driverless AI model predictions are highly linear. The plot above is an implementation of LIME or Local Interpretable Model Agnostic Explanations which often uses linear surrogate models to help reason about the predictions of a complex model.</p>
<p>6. Go to Task 7.</p>
<p><strong>K-LIME Advance Features</strong></p>
<p>7. On the K-LIME plot, change the Cluster to Cluster 13.</p>
<p>8. Select another high probability default person from this K-LIME cluster by clicking on one of the white points on the top-right section of the plot</p>
<p><img alt="k-lime-cluster-13" src="img/e6564bba59f3c786.jpg"></p>
<ol type="1">
<li>Change cluster to cluster 3 and note the R2 value is still very high.</li>
<li>Pick a point on the top-right section of the plot.</li>
<li>Examine Reason Codes.</li>
</ol>
<p>The local model predictions (white points) can be used to reason through the Driverless AI model (yellow) in some local region.</p>
<p>9. Select <strong>Explanations</strong> on the <strong>K-LIME</strong> plot</p>
<p><img alt="reason-codes-for-row-233-1" src="img/ef69cb0292c64bf8.jpg"></p>
<p><img alt="reason-codes-for-row-233-2" src="img/91b73f0aaa2a8f55.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>The reason codes shows that the Driverless model prediction value gave this person .76 percent probability of defaulting. LIME prediction value gave them a .73 percent probability of defaulting and in this case we can say that LIME prediction accuracy is 96.1% accurate. Based on this observation, it can concluded that the local reason codes are fairly trustworthy. If <strong>Lime Prediction Accuracy</strong> drops below 75%, then we can say that the numbers are probably untrustworthy, and the Shapley plot or LOCO plot should be revisited because the Shapley values are always accurate, and LOCO accounts for nonlinearity and interactions.</li>
<li>PAY_0 = 2 months late is the top positive local attribute for this person and contributes .35 probability points to their prediction according to this linear model. 0.35 is the local linear model coefficient for level 3 of the categorical variable PAY_0.</li>
<li>Cluster 13 reason codes show the average linear trends in the data region around this person.</li>
<li>Global reason codes show the average linear trends in the data set as a whole.</li>
</ol>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#k-lime-and-lime-sup" target="_blank">K-LIME and LIME-SUP</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/viewing-explanations.html" target="_blank">Viewing Explanations</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 7: Local Shapley and LOCO" duration="0">
        <h2>Local Shapley Concepts</h2>
<p>Shapley explanations are a technique with credible theoretical support that presents globally consistent global and locally accurate variable contributions. Local numeric Shapley values are calculated by repeatedly tracing single rows of data through a trained tree ensemble and aggregating the contribution of each input variable as the row of data moves through the trained ensemble.</p>
<p>Shapley values sum to the prediction of the Driverless AI model before applying the link function or any regression transforms. (Global Shapley values are the average of the local Shapley values over every row of a data set.)</p>
<h2>LOCO Concepts</h2>
<p>Local feature importance describes how the combination of the learned model rules or parameters and an individual row&#39;s attributes affect a model&#39;s prediction for that row while taking nonlinearity and interactions into effect. Local feature importance values reported here are based on a variant of the leave-one-covariate-out (LOCO) method (Lei et al, 2017).</p>
<p>In the LOCO-variant method, each local feature importance is found by re-scoring the trained Driverless AI model for each feature in the row of interest, while removing the contribution to the model prediction of splitting rules that contain that feature throughout the ensemble. The original prediction is then subtracted from this modified prediction to find the raw, signed importance for the feature. All local feature importance values for the row can be scaled between 0 and 1 for direct comparison with global feature importance values.</p>
<h2>Local Shapley Plot</h2>
<p>Local Shapley Plot is able to generate variable contribution values for every single row that the model predicts on. This means that we can generate for every person in our dataset the exact numeric contribution of each of the variables for each prediction of the model. For the <strong>Local Shapley</strong> plots, the yellow bars stay the same since they are the global variable contribution, however the grey bars will change when a different row or person is selected from the dataset using the row selection dialog box or by clicking on an individual in the K-LIME plot.</p>
<p>The grey bars or local numeric contributions can be used to generated reason codes. The reason codes should be  suitable for regulated industries where modeling decisions need to be justified. For our dataset, we can select a person with a high default probability, select Shapley Local plot and pick out the largest grey bars as the biggest contributors for the decision of denying a future loan.</p>
<p>1. Select a high probability default person on the K-LIME plot by clicking on one of the white points in the top-right corner of the plot</p>
<p>2. Then under the <strong>Driveless AI Model</strong> select <strong>Shapley</strong></p>
<p><img alt="local-shapley-value-plot" src="img/cbe360387b3bc0ca.jpg"></p>
<p><strong>Note:</strong> The Shapley plot will depend on K-LIME point you selected.</p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>Row number being observed: type &#34;11427&#34; and then click &#34;Search&#34;</li>
<li>Global Shapley value</li>
<li>A sample of a Shapley Local  numeric contribution of a variable for the high probability person in row 11427</li>
</ol>
<p>3. Pick another high probability person on the <strong>K-LIME</strong> plot and return to the <strong>Shapley</strong> plot and determine what local Shapley values have influenced the person you selected from defaulting (look for the largest grey bars)?</p>
<p>4. How do those Shapley local values compare to their perspective Shapley Global values? Are the local Shapley values leaning towards defaulting even though the Shapley global values are leaning towards not defaulting? How do the local Shapley values compare to the local K-LIME values?</p>
<p>5. Under <strong>Surrogate Models</strong> select <strong>Decision Tree</strong></p>
<p><img alt="decision-tree-local-value" src="img/f71517cd45717540.jpg"></p>
<p>Based on the <strong>Decision Tree</strong> above, for the high probability person in row 11427, the grey path is consistent with the longest yellow bars in the Shapley Local plot.</p>
<p>6. Is the <strong>Decision Tree</strong> path you selected consistent with the Shapley Local values you noted for question 3?</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#loco" target="_blank">Driverless AI LOCO</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 8: Individual Conditional Expectation" duration="0">
        <h2>The ICE Technique</h2>
<p>Individual conditional expectation (ICE) plots, a newer and less well-known adaptation of partial dependence plots, can be used to create more localized explanations for a single individual using the same basic ideas as partial dependence plots. ICE Plots were described by Goldstein et al (2015). ICE values are simply disaggregated partial dependence, but ICE is also a type of nonlinear sensitivity analysis in which the model predictions for a single row are measured while a variable of interest is varied over its domain. ICE plots enable a user to determine whether the model&#39;s treatment of an individual row of data is outside one standard deviation from the average model behavior, whether the treatment of a specific row is valid in comparison to average model behavior, known standards, domain knowledge, and reasonable expectations, and how a model will behave in hypothetical situations where one variable in a selected row is varied across its domain.</p>
<p>ICE is simply the prediction of the model for the person in question, in our case row 11427. The data for this row was fixed except for PAY_0 then it was cycled through different pay values. The plot above is the result of this cycling. If the ICE values (grey dots) diverge from the partial dependence (yellow dots). In other words, if ICE values are going up and partial dependence going down. This behavior can be indicative of an interaction. The reason for this is that the individual behavior (grey dots) is different since it is diverging from the average behavior.</p>
<h2>ICE Plots</h2>
<p>1. Select <strong>Dashboard</strong></p>
<p><img alt="dashboard" src="img/8a5711acc8577390.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>ICE (grey dots)</li>
<li>Partial Dependence (yellow dots)</li>
<li>LOCO feature Importance for the person in row 11427 (grey bars) in relation to global feature importance</li>
<li>Decision Tree Path for the person in row 11427</li>
</ol>
<p>We can observe divergence on the ICE plot and confirm possible interactions with the surrogate decision tree (the grey path). There seems to be interactions between PAY_0, PAY_6 and PAY_2 when PAY_0 = 3 - 4.</p>
<p>2. <strong>Return to Task 6 question 7</strong> to conclude K-LIME.</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/1810.02909" target="_blank">On the Art and Science of Machine Learning Explanations</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html#the-ice-technique" target="_blank">H2O ICE Technique</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 9: Putting It All Together: Dashboard View" duration="0">
        <p>Using the techniques together to find interactions and other interesting model behavior. What can you find using the Dashboard view?</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://www.h2o.ai/blog/interview-with-patrick-hall-machine-learning-h2o-ai-machine-learning-interpretability/" target="_blank">Machine Learning Interview with Patrick Hall</a>(Feb 2020)</li>
<li><a href="https://www.h2o.ai/blog/interpretability-the-missing-link-between-machine-learning-healthcare-and-the-fda/" target="_blank">Interpretability: The misssing Link between Machine Learning, and the FDA</a>(Aug 2018)</li>
<li><a href="https://www.darpa.mil/program/explainable-artificial-intelligence" target="_blank">Explainable Artificial Intelligence XAI</a></li>
<li><a href="https://docs.google.com/viewer?url=https%3A%2F%2Fwww.darpa.mil%2Fattachments%2FXAIIndustryDay_Final.pptx" target="_blank">Explainable Artificial Intelligence XAI Google Slides</a></li>
<li><a href="https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf" target="_blank">DARPA-BAA-16-53</a> and<br><a href="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf" target="_blank">https://www.darpa.mil/attachments/XAIProgramUpdate.pdf</a></li>
<li><a href="https://github.com/h2oai/mli-resources/blob/master/cheatsheet.png" target="_blank">MLI Cheatsheet</a></li>
<li><a href="https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques" target="_blank">Testing Machine Learning Explanation Techniques</a></li>
<li><a href="https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/72421" target="_blank">Strata Data Conference</a></li>
<li><a href="https://www.youtube.com/watch?v=RcUdUZf8_SU" target="_blank">MLI Meetup before Strata NYC 2018</a></li>
<li><a href="https://www.youtube.com/watch?v=vUqC8UPw9SU" target="_blank">Practical Tips for Interpreting Machine Learning Models - Patrick Hall, H2O.ai  (June 18)</a></li>
<li><a href="https://www.youtube.com/watch?v=Q8rTrmqUQsU" target="_blank">Building Explainable Machine Learning Systems: The Good, the Bad, and the Ugly (May 18)</a></li>
<li><a href="https://www.youtube.com/watch?v=3uLegw5HhYk" target="_blank">Interpretable Machine Learning  (April, 17)</a></li>
<li><a href="https://www.youtube.com/watch?v=Ds1eRF7wpCU&amp=&feature=youtu.be" target="_blank">Ideas on Machine Learning Interpretability</a></li>
<li><a href="https://www.youtube.com/watch?v=axIqeaUhow0" target="_blank">Driverless AI Hands-On Focused on Machine Learning Interpretability - H2O.ai (Dec 17)</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Next Steps" duration="0">
        <p>Check out the next tutorial: <a href="https://training.h2o.ai/products/tutorial-2a-time-series-recipe-tutorial-retail-sales-forecasting" target="_blank">Time Series Tutorial - Retail Sales Forecasting</a> where you will learn more about:</p>
<ul>
<li>Time-series:<br><br><ul>
<li>Time-series concepts</li>
<li>Forecasting</li>
<li>Experiment settings</li>
<li>Experiment results summary</li>
<li>Model interpretability</li>
<li>Analysis</li>
</ul>
</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="../assets/codelab-elements/native-shim.js"></script>
  <script src="../assets/codelab-elements/custom-elements.min.js"></script>
  <script src="../assets/codelab-elements/prettify.js"></script>
  <script src="../assets/codelab-elements/codelab-elements.js"></script>

</body>
</html>
